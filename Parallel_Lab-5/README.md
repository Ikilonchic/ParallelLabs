# Отчет по лабораторной работе №5

по дисциплине "Параллельные и распредлённые вычисления"
студента группы ПА-18-2
Куца Никиты Юрьевича

## Постановка задачи

* Организовать рассылку информации от одного процесса всем остальным членам какой-то области связи (`MPI_Bcast`);
* Собрать (`MPI_Gather`) распределенные по процессам массивы в один массив с сохранением его в адресном простарнстве выделенного
  (root) процесса (`MPI_Gather`);
* Провести глобальные операции (`sum`, `min`, `max`) над данными, которые расположены в адресном пространстве
  разных процессов: с сохранением результата в адресном пространстве одного процесса (`MPI_Reduce`);
* Организовать выдачу результатов на нулевом процессе.

## Выполнение

В программе используются след. функции библиотеки MPI:

* `MPI_Bcast(void* buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)` - транслирует сообщение от "корневого" процесса ко всем другим:
  * `buffer` - адрес начала расположения в памяти рассылаемых данных;
  * `count` - число посылаемых элементов;
  * `datatype` - тип посылаемых элементов;
  * `root` - номер процесса-отправителя;
  * `comm` - коммуникатор.

* `MPI_Gather(void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)` - собирает вместе сообщения от группы:
  * `sendbuf` -	адрес начала размещения посылаемых данных;
  * `sendcount` - число посылаемых элементов;
  * `sendtype` - тип посылаемых элементов;
  * `recvbuf` - адрес начала буфера приема (используется только в процесcе-получателе root);
  * `recvcount` - число элементов, получаемых от каждого процесса (используется только в процессе-получателе root);
  * `recvtype` - тип получаемых элементов;
  * `root` - номер процесса-получателя;
  * `comm` - коммуникатор.

* `MPI_Reduce(void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm)` - приводит значения на всех процессах к одному:
  * `sendbuf`	-	адрес начала входного буфера;
  * `recvbuf`	-	адрес начала буфера результатов (используется только в процессе-получателе root);
  * `count`	- число элементов во входном буфере;
  * `datatype` - тип элементов во входном буфере;
  * `op` -	операция, по которой выполняется редукция;
  * `root` - номер процесса-получателя результата операции;
  * `comm` - коммуникатор.

Корневой процесс создает массив, разделяет его на равные фрагменты, которые распределяются между процессами.

### Алгоритм глобальных операций

* сумма - каждый процесс находит сумму элементов в своем фрагменте, после чего отправляет ее в корневой, где находится общая сумма:

```C
int Sum(int array[], const int start, const int end) {
    int sum = 0;

    for (int i = start; i < end; ++i) {
        sum += array[i];
    }

    return  sum;
}
```

* минимум - каждый процесс находит минимальный элемент в своем фрагменте, после чего отправляет его в корневой, где находится минимум:

```C
int Min(int array[], const int start, const int end) {
    int min = array[start];

    for (int i = start + 1; i < end; ++i) {
        if (array[i] < min)
            min = array[i];
    }

    return min;
}
```

* максимум - каждый процесс находит максимальный элемент в своем фрагменте, после чего отправляет его в корневой, где находится максимум:

```C
int Max(int array[], const int start, const int end) {
    int max = array[start];

    for (int i = start + 1; i < end; ++i) {
        if (array[i] > max)
            max = array[i];
    }

    return max;
}
```

## Анализ результатов

С помощью MPI можно находить частичный результат операций, а потом выполнять объединение.

![mpi-example][mpi-example]
![mpi-example-2][mpi-example-2]

[mpi-example]: img/mpi-example.png
[mpi-example-2]: img/mpi-example-2.png

Как видим тут тоже присутствует рассинхрон между процессами, они могут выполнятся в любом порядке, но результат всегда истинен.
